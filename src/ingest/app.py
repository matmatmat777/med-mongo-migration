# -*- coding: utf-8 -*-
"""
ü©∫ app.py ‚Äî Migration CSV vers MongoDB avec nettoyage et regroupement

But du script :
---------------
‚û° Lire un fichier CSV contenant les admissions de patients.
‚û° Nettoyer et normaliser les donn√©es (corriger les majuscules, espaces, etc.).
‚û° Regrouper toutes les admissions d‚Äôun m√™me patient dans une seule fiche.
‚û° Sauvegarder le r√©sultat dans un fichier JSON.
‚û° (Optionnel) Ins√©rer ou mettre √† jour les patients dans MongoDB.

Le r√©sultat est donc un JSON et une base Mongo avec une structure :
{
  "Name": "Ashley Garcia",
  "Age": 59,
  "Gender": "Male",
  "Blood Type": "O-",
  "Admissions": [ {...}, {...} ]
}
"""

# =======================================================================
# üß© IMPORTS : biblioth√®ques n√©cessaires
# =======================================================================

import os          # pour lire les variables d‚Äôenvironnement (ex: MONGO_URI)
import json        # pour exporter les donn√©es en JSON
import time        # pour les pauses lors de la connexion Mongo
from pathlib import Path   # pour manipuler les chemins de fichiers facilement
from typing import List, Dict, Any   # pour mieux typer les fonctions (lisibilit√©)

import pandas as pd   # biblioth√®que de manipulation de donn√©es tabulaires (CSV)
from pymongo import MongoClient, UpdateOne   # client MongoDB et op√©rations bulk
from pymongo.errors import ServerSelectionTimeoutError, BulkWriteError   # erreurs Mongo


# =======================================================================
# ‚öôÔ∏è CONFIGURATION & OUTILS DE BASE
# =======================================================================

def load_config():
    """
    Lis les variables d‚Äôenvironnement (ou met des valeurs par d√©faut)
    Ces variables sont d√©finies dans docker-compose.yml.
    """
    CSV_PATH = os.getenv("CSV_PATH", "/data/input/patients.csv")  # chemin du CSV
    REPORTS_DIR = Path(os.getenv("REPORTS_DIR", "/data/reports/migration"))  # dossier o√π √©crire le JSON
    MONGO_URI = os.getenv("MONGO_URI", "mongodb://root:example@mongo:27017/?authSource=admin")
    DB_NAME = os.getenv("MONGO_DB") or os.getenv("MONGO_INITDB_DATABASE", "meddb")
    ENABLE_MONGO_INSERT = os.getenv("ENABLE_MONGO_INSERT", "1") == "1"  # activer l‚Äôinsertion Mongo ?
    return CSV_PATH, REPORTS_DIR, MONGO_URI, DB_NAME, ENABLE_MONGO_INSERT


def get_client(mongo_uri: str) -> MongoClient:
    """Cr√©e une connexion MongoDB (client)."""
    return MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)


def wait_for_mongo(client: MongoClient, retries: int = 30, delay: int = 2) -> None:
    """
    Attends que Mongo soit pr√™t avant d‚Äôins√©rer.
    - Essaie un 'ping' jusqu‚Äô√† 30 fois (toutes les 2 secondes).
    """
    for _ in range(retries):
        try:
            client.admin.command("ping")  # si Mongo r√©pond, c‚Äôest pr√™t
            print("‚úÖ MongoDB connect√©")
            return
        except ServerSelectionTimeoutError:
            print(f"‚è≥ MongoDB non pr√™t, retry dans {delay}s...")
            time.sleep(delay)
    raise RuntimeError("‚ùå Impossible de se connecter √† MongoDB")


# =======================================================================
# üßπ LECTURE ET R√âSUM√â DU FICHIER CSV
# =======================================================================

def read_data(path: str) -> pd.DataFrame:
    """Lit un fichier CSV ou JSON et renvoie un DataFrame pandas."""
    print(f"üìÑ Lecture: {path}")
    if path.lower().endswith(".json"):
        return pd.read_json(path)
    return pd.read_csv(path, low_memory=False)

def summarize(df: pd.DataFrame) -> None:
    """Affiche des infos utiles sur le jeu de donn√©es."""
    print("\n‚ÑπÔ∏è  Infos dataset:")
    df.info()
    print("\nüîé Valeurs manquantes:\n", df.isnull().sum())
    print("\nüßÆ Doublons (lignes strictes):", df.duplicated().sum())


# =======================================================================
# üß† NORMALISATION DES CHAMPS
# =======================================================================

def _clean_str(x: str) -> str:
    """Nettoie les cha√Ænes : supprime les espaces, tabulations, doublons, etc."""
    if pd.isna(x):
        return ""
    s = " ".join(str(x).strip().replace("\t", " ").split())
    s = s.replace(" ,", ",").replace(", ,", ",").strip()
    return s

def normalize_name(raw: str) -> str:
    """Met les noms en 'Title Case' (Ashley Garcia, pas ASHLEY GARCIA)"""
    s = _clean_str(raw)
    if not s:
        return ""
    parts = [p for p in s.replace(",", " ").split(" ") if p]
    parts = [p.lower().capitalize() if len(p) > 1 else p.upper() for p in parts]
    # Correction sp√©ciale pour les noms commen√ßant par "Mc"
    fixed = []
    for p in parts:
        if p.startswith("Mc") and len(p) > 2:
            p = "Mc" + p[2:].capitalize()
        fixed.append(p)
    return " ".join(fixed)

def normalize_gender(raw: str) -> str:
    """Uniformise les genres ('Male' ou 'Female')."""
    s = _clean_str(raw).lower()
    if s in ("male", "m", "masculin"):
        return "Male"
    if s in ("female", "f", "feminin", "f√©minin"):
        return "Female"
    return s.capitalize() if s else ""

def normalize_blood(raw: str) -> str:
    """Met les groupes sanguins en forme (A+, B-, O-, etc.)"""
    s = _clean_str(raw).upper().replace(" ", "")
    valid = {"A","A+","A-","B","B+","B-","O","O+","O-","AB","AB+","AB-"}
    return s if s in valid else s


# =======================================================================
# üîÑ TRANSFORMATION DES DONN√âES
# =======================================================================

GROUP_COLS_NORM = ["Name_norm", "Age", "Gender_norm", "Blood_norm"]

def clean(df: pd.DataFrame) -> pd.DataFrame:
    """Supprime les doublons stricts (lignes identiques)."""
    return df.drop_duplicates()

def group_patients(df: pd.DataFrame) -> pd.DataFrame:
    """
    Regroupe les lignes par patient (apr√®s normalisation des champs cl√©s).
    Chaque groupe devient un patient avec plusieurs admissions possibles.
    """
    df = df.copy()
    df["Name_norm"]   = df["Name"].apply(normalize_name)
    df["Gender_norm"] = df["Gender"].apply(normalize_gender)
    df["Blood_norm"]  = df["Blood Type"].apply(normalize_blood)

    # On garde toutes les colonnes sauf celles utilis√©es pour le groupement
    admission_cols = [c for c in df.columns if c not in ["Name","Age","Gender","Blood Type"] + GROUP_COLS_NORM]

    grouped = (
        df.groupby(GROUP_COLS_NORM, dropna=False)[admission_cols]
          .apply(lambda x: x.to_dict(orient="records"))  # liste de dicts = admissions
          .reset_index(name="Admissions")
    )

    # On remet des noms propres pour l‚Äôexport final
    grouped["Name"] = grouped["Name_norm"]
    grouped["Gender"] = grouped["Gender_norm"]
    grouped["Blood Type"] = grouped["Blood_norm"]

    return grouped[["Name","Age","Gender","Blood Type","Admissions"]]


def make_docs(grouped: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    Convertit les groupes pandas en documents JSON.
    - D√©duplication stricte des admissions identiques
    - Tri des admissions par Date of Admission
    """
    docs: List[Dict[str, Any]] = []
    for _, row in grouped.iterrows():
        admissions = list(row["Admissions"])

        # On cr√©e une "signature" pour rep√©rer les doublons exacts d‚Äôadmission
        seen = set()
        unique_adm = []
        for a in admissions:
            sig = tuple(_clean_str(str(v)) for v in a.values())
            if sig in seen:
                continue
            seen.add(sig)
            unique_adm.append(a)

        # Tri chronologique des admissions
        def _adm_key(a):
            return str(a.get("Date of Admission","")) or ""

        unique_adm.sort(key=_adm_key)

        doc = {
            "Name": row["Name"],
            "Age": int(row["Age"]) if pd.notna(row["Age"]) else None,
            "Gender": row["Gender"],
            "Blood Type": row["Blood Type"],
            "Admissions": unique_adm
        }
        docs.append(doc)
    return docs


# =======================================================================
# üß™ DEBUG : d√©tecter les patients √† plusieurs admissions
# =======================================================================

def debug_multi_admissions_from_csv(df: pd.DataFrame, reports_dir: Path) -> None:
    """
    V√©rifie s‚Äôil existe des patients qui apparaissent plusieurs fois dans le CSV
    (donc plusieurs admissions).
    """
    tmp = df.copy()
    tmp["Name_norm"]   = tmp["Name"].apply(normalize_name)
    tmp["Gender_norm"] = tmp["Gender"].apply(normalize_gender)
    tmp["Blood_norm"]  = tmp["Blood Type"].apply(normalize_blood)

    # Compte combien de fois chaque patient normalis√© appara√Æt
    grp_cols = ["Name_norm", "Age", "Gender_norm", "Blood_norm"]
    counts = tmp.groupby(grp_cols, dropna=False).size().reset_index(name="rows")

    multi = counts[counts["rows"] > 1]
    print(f"üî¨ DEBUG CSV ‚Äî groupes>1: {len(multi)} patients multi-admissions")
    if len(multi) > 0:
        sample_path = reports_dir / "multi_admissions_sample.csv"
        tmp.merge(multi, on=grp_cols, how="inner").to_csv(sample_path, index=False)
        print(f"üß™ Exemple √©crit: {sample_path.resolve()}")


# =======================================================================
# üíæ EXPORT ET INSERTION DANS MONGO
# =======================================================================

def export_patients_json(docs: List[Dict[str, Any]], reports_dir: Path) -> Path:
    """Sauvegarde tous les patients au format JSON."""
    reports_dir.mkdir(parents=True, exist_ok=True)
    out = reports_dir / "patients.json"
    with open(out, "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2)
    print(f"üìù Export √©crit: {out.resolve()}")
    return out


def upsert_mongo(docs: List[Dict[str, Any]], client: MongoClient, db_name: str, coll_name: str = "patients") -> None:
    """
    Met √† jour ou ins√®re chaque patient dans MongoDB.
    La cl√© unique est bas√©e sur (Name, Age, Gender, Blood Type).
    """
    col = client[db_name][coll_name]
    ops = []

    for d in docs:
        filt = {
            "Name": d.get("Name"),
            "Age": d.get("Age"),
            "Gender": d.get("Gender"),
            "Blood Type": d.get("Blood Type"),
        }
        ops.append(UpdateOne(filt, {"$set": d}, upsert=True))

    if not ops:
        print("‚ö†Ô∏è Aucun document √† ins√©rer.")
        return

    try:
        res = col.bulk_write(ops, ordered=False)
        upserts = len(getattr(res, "upserted_ids", {}) or {})
        print(f"‚úÖ Upserts: {upserts}, Modifi√©s: {res.modified_count}, Mis √† jour (matched): {res.matched_count}")
    except BulkWriteError as e:
        print("‚ùå Erreur BulkWrite:", json.dumps(e.details, indent=2))


# =======================================================================
# üöÄ MAIN : encha√Ænement complet
# =======================================================================

def main():
    # 1Ô∏è‚É£ Charger la config
    CSV_PATH, REPORTS_DIR, MONGO_URI, DB_NAME, ENABLE_MONGO_INSERT = load_config()

    # 2Ô∏è‚É£ Lire le CSV et r√©sumer
    df = read_data(CSV_PATH)
    print("‚úÖ Lecture termin√©e")
    summarize(df)

    # 3Ô∏è‚É£ Supprimer les doublons stricts
    df = clean(df)
    print("‚úÖ Nettoyage termin√©")

    # 4Ô∏è‚É£ V√©rifier s‚Äôil y a des multi-admissions
    debug_multi_admissions_from_csv(df, REPORTS_DIR)

    # 5Ô∏è‚É£ Regrouper et construire les docs
    grouped = group_patients(df)
    print(f"‚úÖ Groupement termin√© -> {len(grouped)} patients uniques")
    docs = make_docs(grouped)
    print(f"üì¶ Documents pr√™ts: {len(docs)}")

    # 6Ô∏è‚É£ Export JSON
    export_patients_json(docs, REPORTS_DIR)

    # 7Ô∏è‚É£ Envoi vers MongoDB (optionnel)
    if ENABLE_MONGO_INSERT:
        client = get_client(MONGO_URI)
        wait_for_mongo(client)
        upsert_mongo(docs, client, DB_NAME, "patients")
    else:
        print("‚ÑπÔ∏è Insertion Mongo d√©sactiv√©e.")

# Point d‚Äôentr√©e du script
if __name__ == "__main__":
    main()
